<!-- # 数理统计

## 1. 总体与样本

concepts

- 简单随机样本
- 样本空间
- 独立同分布
- 联合分布函数
- 统计量
- 样本均值

examples

- 正态分布
- 二项分布
- Bernoulli 分布
- Poi
- exp
- Gamma
- Cauchy
- 渐近分布*
- 均匀分布
- Beta 分布

## 2

concepts

- 样本方差、标准差、样本方差的期望
- 样本矩（原点、中心）
- 样本变异系数
- 样本偏度
- 样本峰度
- 样本极差
- 中列数/中矩
- 中位数
- 样本 p 分位数
- 总体 p 分位数
- 样本累积分布函数（经验函数）

---

- 顺序统计量
- 顺序统计量的联合分布函数

prop

- $Y_\alpha$ 的分布函数

## 3. 来自正态总体的抽样

### 卡方分布

concepts

- 密度函数
- 期望
- 方差
- 自由度
- 矩母函数
- 特征函数

prop

- 矩母函数的性质？
- 是特殊的 Gamma 函数，因此具有可加性
- 标准正态分布总体的抽样的平方和服从卡方分布

---

- 均值的分布
- 均值与方差独立
- 方差经过简单的变换后满足卡方分布 -->

## 第三章 参数估计

>什么是参数估计？

若**已知某个随机变量满足某种分布**，但不知道这个分布中某些参数的取值，我们可以用若干次测量的结果对这个参数进行**近似**估计。

举例：小明去射箭，十次射击中命中了7次，估计小明的命中率。显然，每次射箭的结果满足 Bernoulli 分布，命中率为未知参数 p, 则通过十次射击的结果，可以用 $\sum\limits_{i=1}^{10} \frac{X_i}{10}$ 来估算命中率，即 $\overline{X}_n$. 

>除了用平均值来估计，还有没有其他的估计方法呢？

常用的估计方法有：

- 矩方法
- 极大似然法
- 贝叶斯方法

>为什么要用这些方法估计呢？

我想，应该是因为他们的估计结果和实际有着非常好的近似程度。

>如何来衡量这个近似程度呢？

~~我的意思是，用近似参数可以给出一个确定的分布，然后再通过多次实验去验证这个分布与实际是否吻合？~~（通常在数理统计中，不采用这种方法）

或者我们还有其他的方法去检验近似程度（也就是参数估计的合理性）？

暂时把这些问题留在这里。

- 无偏性

### 3.1 矩方法

用样本矩来估计总体矩。

$$E X^r = \sum\limits_{i = 1}^n \frac{X_i^r}{n},\quad r = 1,2,\cdots.$$

观察上面的例子，其使用的估计式其实就是一阶矩方法。

>矩方法的优势是什么？

- 相比 MLE，矩方法的方程通常比较简单
- 但是矩方法的**核心优势**是**需要的信息量少**，即只需知道不同阶的矩，而无需知道分布函数，即可对参数进行估计。
- 具有一致性；

>实际中在什么情况下，我们能知道总体的若干阶矩，但是不知道分布函数呢？

这个问题过于复杂，牵涉到过多其他学科的例子。

### 3.2 极大似然法

写出若干个样本的 j.c.d.f. 或者 j.p.d.f, 寻找使这个函数达到最大值时参数的取值。更通俗一点的说，这个联合分布函数代表的是一组测量结果对应的概率，那当然，使得这个结果发生的概率最大时对应的参数取值应该就可以作为参数的一个近似估计。

### 3.3 衡量近似程度

在之前，我提出过问题：如何衡量一个参数估计方法是否良好？可以用下面的一些标准来检验。

#### 无偏性

点估计可以视为是一个随机变量，因此可以计算其期望。如果 $E \hat{\theta} = \theta$，则认为这个估计具有无偏性。

减弱一些条件，可以得到**渐近无偏性**。